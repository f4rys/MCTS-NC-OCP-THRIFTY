\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[polish]{babel}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[backend=biber]{biblatex}
\addbibresource{literature.bib}
\usepackage{geometry}
\usepackage{float}
\geometry{margin=1in}

\title{Analiza i eksperymenty nad zrównoleglonym algorytmem MCTS-NC}
\author{Wojciech Bartoszek \\ Jarosław Kołdun}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Niniejszy raport stanowi sprawozdanie z analizy i testów oprogramowania MCTS-NC (Monte Carlo Tree Search - numba.cuda). Dokument opisuje cel rozwiązania, zastosowany model zrównoleglenia GPU oraz narzędzia implementacyjne. Przedstawiono również wyniki własnych eksperymentów dla wariantu \texttt{OCP\_THRIFTY} w grze Connect4, badając wpływ liczby drzew i playoutów na wydajność i jakość decyzji. Dodatkowo przeprowadzono analizę wydajności przy pomocy systemu monitorującego.
\end{abstract}

\section{Przeznaczenie rozwiązania}
Omawiane rozwiązanie, MCTS-NC, jest biblioteką służącą do realizacji algorytmu Monte Carlo Tree Search (MCTS) w środowisku wysoce zrównoleglonym. MCTS to algorytm uczenia ze wzmocnieniem (Reinforcement Learning), który buduje asymetryczne drzewo gry poprzez selektywne próbkowanie akcji i estymację ich wartości na podstawie losowych symulacji (playouts)~\cite{KLESK2025102139}.

Główne obszary zastosowań tego rozwiązania to:
\begin{itemize}
    \item \textbf{Gry decyzyjne:} Szachy, Go, Connect4, Gomoku (weryfikowane w artykule źródłowym).
    \item \textbf{Bioinformatyka:} np. modelowanie 3D chromatyny czy zwijanie RNA.
    \item \textbf{Inżynieria ruchu i energetyka:} optymalizacja przepływu ruchu, zarządzanie siecią energetyczną.
    \item \textbf{Matematyka:} odkrywanie algorytmów mnożenia macierzy.
\end{itemize}
Celem MCTS-NC jest dostarczenie szybkiej, działającej wyłącznie na GPU implementacji, która eliminuje wąskie gardła związane z przesyłaniem danych między CPU a GPU.

\section{Opis zrównoleglenia algorytmu}
Algorytm został zrównoleglony w oparciu o model obliczeniowy \textbf{CUDA (Compute Unified Device Architecture)} Implementacja łączy trzy poziomy zrównoleglenia (Leaf, Root, Tree parallelization) w jeden spójny system.

\subsection{Model podziału i mapowanie wątków}
Podział pracy pomiędzy wątki GPU zależy od etapu algorytmu i wybranego wariantu:
\begin{itemize}
    \item \textbf{Organizacja bloków:} Wątki są grupowane w bloki CUDA, które są indeksowane albo przez same indeksy drzew, albo przez pary (drzewo, akcja).
    \item \textbf{Etapy MCTS:} Wszystkie cztery etapy algorytmu (selekcja, ekspansja, symulacja/playout, wsteczna propagacja/backup) wykorzystują wiele wątków GPU.
    \item \textbf{Redukcje:} Do sumowania wyników oraz wyznaczania wartości maksymalnych (max/argmax) zastosowano wzorce redukcji (reduction patterns), co pozwala na obliczenia w czasie $O(1)$ lub $O(\log n)$.
\end{itemize}

\subsection{Zarządzanie pamięcią i komunikacja}
Implementacja jest typu \textbf{lock-free} (bez blokad) – nie używa operacji atomowych ani muteksów, co jest kluczowe dla wydajności przy masowej równoległości.
\begin{itemize}
    \item \textbf{Współpraca wątków:} Wykorzystano mechanizm kooperacji wątków (threads cooperation) oraz szybką pamięć współdzieloną (shared memory) wewnątrz bloków.
    \item \textbf{Transfer danych:} Zminimalizowano komunikację Host-Device. W wariancie \emph{Prodigal} transfery pamięci są niemal całkowicie wyeliminowane w głównej pętli.
\end{itemize}

\subsection{Warianty algorytmiczne}
Zaimplementowano cztery warianty różniące się sposobem alokacji zasobów:
\begin{itemize}
    \item \textbf{OCP (One-Child Playouts) vs ACP (All-Children Playouts):} W OCP symulowany jest jeden losowy potomek liścia, w ACP – wszyscy potomkowie równolegle.
    \item \textbf{Thrifty vs Prodigal:}
        \begin{itemize}
            \item \textbf{Thrifty (Oszczędny):} Liczba bloków jest dokładnie dopasowana do liczby legalnych akcji. Wymaga to jednak transferów pamięci do hosta w celu przeliczenia indeksów.
            \item \textbf{Prodigal (Rozrzutny):} Alokuje nadmiarową siatkę bloków $(T, B)$. Marnuje pewne zasoby GPU, ale unika kosztownych transferów pamięci Host-Device.
        \end{itemize}
\end{itemize}

\section{Narzędzia realizacyjne}
Do realizacji równoległości wykorzystano następujące technologie:
\begin{itemize}
    \item \textbf{Język:} Python (wersja $\ge 3.13$).
    \item \textbf{Biblioteka równoległości:} \textbf{Numba} (moduł \texttt{numba.cuda}). Jest to kompilator JIT (Just-In-Time), który tłumaczy kod Pythona na instrukcje PTX wykonywalne na kartach graficznych NVIDIA.
    \item \textbf{Sprzęt:} RTX 1650; CUDA 12.8.
\end{itemize}
W przeciwieństwie do rozwiązań opartych na MPI czy OpenMP działających na CPU, użycie \texttt{numba.cuda} pozwala na bezpośrednie pisanie jąder (kernels) CUDA w Pythonie.
 
\section{Przebieg eksperymentów}
Celem eksperymentów (demo) było ocenienie zachowania wariantu \texttt{OCP\_THRIFTY} dla gry Connect4 w różnych konfiguracjach: \texttt{n\_trees} = \{1,4,8,16\} i \texttt{n\_playouts} = \{32,64,128,256,512\}. Każda konfiguracja została wielokrotnie testowana (100 prób).

\subsection{Metodologia eksperymentalna}
\begin{itemize}
  \item Wariant algorytmu: \texttt{OCP\_THRIFTY}
  \item Gra: Connect 4
  \item Miary: \texttt{playouts\_per\_second}, \texttt{best\_q} (średnia wygranych dla wybranej akcji).
\end{itemize}

\subsection{Analiza wydajności i monitoring}
W ramach projektu przeprowadzono analizę wydajności aplikacji przy użyciu systemu monitorującego. Poniżej przedstawiono plan, proponowane metryki oraz wyniki pomiarów z instrumentacji.

\subsubsection{Cele i metryki}
Analiza skupi się na następujących metrykach: wykorzystanie GPU (\%), wykorzystanie pamięci GPU, wykorzystanie CPU (\%), zużycie pamięci RAM procesu, liczba playoutów na sekundę (playouts/s), czasy etapów (select/expand/playout/backup), transfery host<->device oraz, o ile to możliwe, czasy poszczególnych kernelów.

\subsubsection{Plan eksperymentów i wyniki} 
Po uruchomieniu eksperymentów z instrumentacją wygenerowano poniższą tabelę podsumowującą miary monitoringu oraz wykres wykorzystania GPU. \\

\begin{tabular}{lrr}
\toprule
metric & mean & std / max \\
\midrule
CPU (\% ) & 18.8 & 6.5\\
GPU util (\% ) & 51.9 & 4.9\\
GPU mem mean (MB) & 1846.6 & max 2339.4\\
RSS mem mean (MB) & 438.2 & \\
\bottomrule
\end{tabular} \\

\noindent Wnioski: GPU było umiarkowanie obciążone podczas eksperymentów; rozważamy optymalizację parametrów bloków oraz redukcję transferów host $\leftrightarrow$ device w celu lepszego wykorzystania dostępnych zasobów GPU.

\subsubsection{Proponowane ulepszenia — hipotezy do weryfikacji}
\begin{itemize}
  \item Zredukowanie transferów host$\leftrightarrow$device i minimalizacja kopiowania danych (oczekiwane zwiększenie przepustowości i zmniejszenie opóźnień).
  \item Dopasowanie parametrów bloków/wątków (tpb) oraz rozkładu pracy w kernelach w celu lepszego wykorzystania SM GPU.
  \item Usprawnienie generatora losowości (np. pre-allocacja stanów RNG) i optymalizacja inicjalizacji per-thread, aby uniknąć kosztów w pętli programu.
  \item Profilowanie kernelów i rozważenie łączenia prostych kernelów (kernel fusion) lub przenoszenia części logiki na host w celu redukcji liczby wywołań kerneli.
\end{itemize}

\subsection{Zestawienie wyników}
Poniżej wstawiono tabelę podsumowującą średnie wartości (\emph{mean} $\pm$ \emph{std}) dla szybkości playoutów i jakości akcji.\newline
\begin{tabular}{rrrrr}
\toprule
n\_trees & n\_playouts & playouts/s (mean $\pm$ std) & best\_q (mean $\pm$ std) & trials \\
\midrule
1 & 64 & 52,265 $\pm$ 5,265 & 0.6416 $\pm$ 0.0302 & 100 \\
1 & 128 & 100,643 $\pm$ 8,357 & 0.6356 $\pm$ 0.0257 & 100 \\
1 & 256 & 185,010 $\pm$ 11,775 & 0.6337 $\pm$ 0.0229 & 100 \\
1 & 512 & 350,306 $\pm$ 23,495 & 0.6399 $\pm$ 0.0176 & 100 \\
4 & 32 & 101,651 $\pm$ 7,923 & 0.6407 $\pm$ 0.0219 & 100 \\
4 & 64 & 212,774 $\pm$ 15,353 & 0.6389 $\pm$ 0.0187 & 100 \\
4 & 128 & 432,024 $\pm$ 31,429 & 0.6402 $\pm$ 0.0126 & 100 \\
4 & 256 & 819,187 $\pm$ 53,015 & 0.6405 $\pm$ 0.0122 & 100 \\
4 & 512 & 1,346,204 $\pm$ 99,740 & 0.6398 $\pm$ 0.0079 & 100 \\
8 & 32 & 215,212 $\pm$ 15,660 & 0.6379 $\pm$ 0.0195 & 100 \\
8 & 64 & 398,186 $\pm$ 47,392 & 0.6393 $\pm$ 0.0132 & 100 \\
8 & 128 & 778,756 $\pm$ 88,164 & 0.6396 $\pm$ 0.0098 & 100 \\
8 & 256 & 1,495,930 $\pm$ 118,487 & 0.6411 $\pm$ 0.0083 & 100 \\
8 & 512 & 2,296,589 $\pm$ 133,618 & 0.6407 $\pm$ 0.0059 & 100 \\
16 & 32 & 423,327 $\pm$ 31,679 & 0.6387 $\pm$ 0.0131 & 100 \\
16 & 64 & 777,088 $\pm$ 71,423 & 0.6396 $\pm$ 0.0092 & 100 \\
16 & 128 & 1,620,980 $\pm$ 101,277 & 0.6398 $\pm$ 0.0066 & 100 \\
16 & 256 & 2,842,605 $\pm$ 185,688 & 0.6416 $\pm$ 0.0060 & 100 \\
16 & 512 & 3,712,266 $\pm$ 170,840 & 0.6409 $\pm$ 0.0037 & 100 \\
\bottomrule
\end{tabular}

\subsection{Najważniejsze obserwacje}
\begin{itemize}
  \item Najwyższą średnią liczbę playoutów na sekundę zaobserwowano dla konfiguracji \texttt{n\_trees=16, n\_playouts=512}.
  \item Najwyższą średnią jakość decyzji (\texttt{best\_q}) uzyskano dla \texttt{n\_trees=1, n\_playouts=32} (średnia około 0.6496).
  \item Ogólnie: szybkość playoutów rośnie wraz ze wzrostem \texttt{n\_trees} i/lub \texttt{n\_playouts}, natomiast średnia jakość (best\_q) jest względnie stabilna w mierzonych konfiguracjach.
\end{itemize}

\subsection{Wykresy}
Poniżej zamieszczono kluczowe wykresy wygenerowane podczas eksperymentów.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{../plots/playouts_per_second_vs_n_playouts.png}
  \caption{Średnie playouts/sec w zależności od $n\_playouts$ dla różnych wartości $n\_trees$.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{../plots/best_q_vs_n_playouts.png}
  \caption{Średnie $best\_q$ w zależności od $n\_playouts$ dla różnych wartości $n\_trees$.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{../plots/best_q_boxplot.png}
  \caption{Boxploty rozkładu $best\_q$ dla różnych konfiguracji $n\_trees$ i $n\_playouts$.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{../plots/gpu_utilization_over_time.png}
  \caption{Wykres wykorzystania GPU w czasie podczas eksperymentu.}
\end{figure}

\section{Dyskusja i propozycja ulepszenia}
Wyniki eksperymentów wskazują, że samo zwiększanie przepustowości (liczby playoutów na sekundę) nie przekłada się liniowo na jakość decyzji w wariancie OCP. W związku z tym, zamiast prostej zmiany wariantu na bardziej zasobożerny (ACP), proponujemy optymalizację algorytmiczną.

\subsection{Propozycja: Dynamiczne skalowanie liczby playoutów}
Zgodnie z otwartymi pytaniami badawczymi postawionymi przez autora oprogramowania, proponujemy implementację mechanizmu \textbf{zmiennej liczby playoutów w zależności od głębokości drzewa}.

\begin{itemize}
    \item \textbf{Problem:} Obecnie parametr $m$ (liczba playoutów) jest stały dla każdego węzła. Powoduje to, że zasoby GPU są zużywane równomiernie, nawet w sytuacjach, gdzie ruchy są oczywiste lub mało znaczące dla wyniku końcowego.
    \item \textbf{Rozwiązanie:} Modyfikacja jądra \texttt{\_playout\_ocp} tak, aby parametr $m$ był funkcją głębokości węzła $d$, np. $m(d) = m_{base} \cdot f(d)$.
    \item \textbf{Oczekiwany efekt:} Pozwoli to na "głębsze" przeszukiwanie w krytycznych momentach gry przy zachowaniu tego samego budżetu czasowego, co powinno podnieść wskaźnik wygranych bez konieczności drastycznego zwiększania pamięci (jak w wariancie Prodigal).
    \item \textbf{Plan wdrożenia:} Wprowadzenie tablicy wag dla głębokości przesyłanej do pamięci stałej (constant memory) GPU i skalowanie pętli symulacji wewnątrz kernela CUDA.
\end{itemize}

\renewcommand{\refname}{Literatura}
\nocite{*}
\printbibliography

\end{document}